{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6205ed",
   "metadata": {},
   "source": [
    "### Inspect adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "faec176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.getenv(\"RESNET_MODEL_FOLDER\")\n",
    "print(model_folder)\n",
    "\n",
    "target_class = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6276db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nadf.data.adversarial import load_or_create_dataset\n",
    "\n",
    "dataset = load_or_create_dataset(\n",
    "    folder=model_folder,\n",
    "    target_class=target_class,\n",
    "    num_attacks_eps_coef=[(4, 0.25), (2, 0.5), (3, 1.0), (1, 2.0)],\n",
    "    recreate=False,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ea1b921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4361b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e456465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"y\"][\"train\"].unique()\n",
    "dataset[\"y\"][\"val\"].unique()\n",
    "dataset[\"y\"][\"test\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6bdd8",
   "metadata": {},
   "source": [
    "### Make regression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0f2b9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nadf.data.datasets import create_regression_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "abedd893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "regression_datasets = {}\n",
    "clean_upweight_factor = 3.0\n",
    "\n",
    "\n",
    "# def create_regression_dataset(\n",
    "#     z_all,\n",
    "#     y_all,\n",
    "#     z_clean,\n",
    "#     y_clean,\n",
    "#     distance_metric=\"euclidean\",\n",
    "#     clean_upweight_factor=1.0,\n",
    "#     z_clean_pool=None,\n",
    "#     y_clean_pool=None,\n",
    "#     split=\"train\",\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]: \n",
    "    y_adv_for_matching = torch.full((len(dataset[\"y_adv\"][split]),), target_class)  # Override to all 9s # Use 9s, not true labels\n",
    "    print(y_adv_for_matching)\n",
    "    y_adv = y_adv_for_matching\n",
    "    \n",
    "    if target_class == -1:\n",
    "        \n",
    "        y_adv = dataset[\"y_adv\"][split]\n",
    "        print(y_adv)\n",
    "    # regression_datasets[split] = create_regression_dataset(\n",
    "    #     z_all = dataset[\"z\"][split],\n",
    "    #     y_all = y_all_for_matching,\n",
    "    #     z_clean = dataset[\"z_clean\"][split],\n",
    "    #     y_clean = dataset[\"y_clean\"][split],\n",
    "    #     z_clean_pool = dataset[\"z_clean_pool\"][split],\n",
    "    #     y_clean_pool = dataset[\"y_clean_pool\"][split])\n",
    "\n",
    "    regression_datasets[split] = create_regression_dataset(\n",
    "        z_clean = dataset[\"z_clean\"][split],\n",
    "        y_clean = dataset[\"y_clean\"][split],\n",
    "        z_adv = dataset[\"z_adv\"][split],\n",
    "        #y_adv = dataset[\"y_adv\"][split],\n",
    "        y_adv=y_adv,  \n",
    "        clean_upweight_factor=clean_upweight_factor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c9cbbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# After creating the regression dataset\n",
    "regression_dataset_path = os.path.join(model_folder, \"adversarial_examples\", str(target_class), \"regression_dataset.pt\")\n",
    "\n",
    "# Save the dataset\n",
    "os.makedirs(os.path.dirname(regression_dataset_path), exist_ok=True)\n",
    "torch.save(regression_datasets, regression_dataset_path)\n",
    "print(f\"Saved regression dataset to {regression_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "da8110c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchors, distances, labels, upweights\n",
    "\n",
    "# check for unique values in distances\n",
    "regression_datasets[\"train\"][1].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c92a9",
   "metadata": {},
   "source": [
    "## train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f23468fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import dotenv\n",
    "from argparse import Namespace\n",
    "from nadf.training.pipeline import train_probe_model\n",
    "from typing import Dict, Any\n",
    "import os, glob, torch\n",
    "from nadf.training.pipeline import load_probe_model\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "model_folder = os.getenv(\"RESNET_MODEL_FOLDER\")\n",
    "print(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "acd9f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_dataset_path = os.path.join(model_folder, \"adversarial_examples\", str(target_class), \"regression_dataset.pt\")\n",
    "regression_datasets = torch.load(regression_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6a38f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "\n",
    "# Check for target distances = 0\n",
    "exact_zeros = regression_datasets[\"train\"][1] == 0\n",
    "num_exact_zeros = (regression_datasets[\"train\"][1] == 0).sum()\n",
    "\n",
    "num_exact_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b198a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchors, distances, labels, upweights\n",
    "regression_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "288866e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]: \n",
    "    anchors, distances, labels, sample_weights = regression_datasets[split]\n",
    "    # anchors, distances, labels, sample_weights = create_regression_dataset(\n",
    "    #     z_all = dataset[\"z\"][split],\n",
    "    #     y_all = torch.full((len(dataset[\"y\"][split]),), 9),  # All 9s for matching # ! NOT SURE THIS IS VALID\n",
    "    #     z_clean = dataset[\"z_clean\"][split],\n",
    "    #     y_clean = dataset[\"y_clean\"][split],\n",
    "    #     z_clean_pool = dataset[\"z_clean_pool\"][split],\n",
    "    #     y_clean_pool = dataset[\"y_clean_pool\"][split])\n",
    "    \n",
    "    # Convert tuple to dictionary\n",
    "    datasets[split] = {\n",
    "        \"anchors\": anchors,\n",
    "        \"distances\": distances,\n",
    "        \"labels\": labels,\n",
    "        \"sample_weights\": sample_weights\n",
    "    }\n",
    "\n",
    "\n",
    "input_dim = anchors.shape[1]\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "# Now you can train\n",
    "args = Namespace(\n",
    "    input_dim=input_dim, \n",
    "    depth=5, \n",
    "    width=1024, \n",
    "    activation=\"relu\",\n",
    "    loss=\"huber\",\n",
    "    batch_size=256,\n",
    "    model_type=\"mlp\",\n",
    "    epochs=30,\n",
    "    lr=0.001,\n",
    "    weight_decay=0.0001,\n",
    "    verbose=True,\n",
    "    num_epochs=30,  \n",
    "    huber_delta=1.0,  \n",
    "    checkpoint_name=None,  # Auto-generate\n",
    "    save_dir=\"./checkpoints\",\n",
    "    upweight=clean_upweight_factor,  # Clean upweight factor (1.0 = no upweighting)\n",
    "    augmentation=\"none\",  # No augmentation\n",
    "    num_augmentations=0,  # Number of augmentations\n",
    "    target_class=target_class,\n",
    ")\n",
    "\n",
    "# Create the save directory\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "checkpoint_path, final_metrics = train_probe_model(datasets, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20082fd0",
   "metadata": {},
   "source": [
    "## evaluate probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8dca5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d96c5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"./checkpoints/mlp_d3w512_mse_up3.0x.pth\"\n",
    "checkpoint_path = checkpoint_path\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Check what's in the checkpoint\n",
    "print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "print(\"\\nModel config keys:\", checkpoint['model_config'].keys())\n",
    "print(\"\\nTraining config keys:\", checkpoint['training_config'].keys())\n",
    "\n",
    "# Print some key values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKPOINT DETAILS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(f\"Total epochs trained: {checkpoint['training_config']['num_epochs']}\")\n",
    "print(f\"Test Loss: {checkpoint['training_config']['final_test_loss']:.4f}\")\n",
    "print(f\"Val Loss: {checkpoint['training_config']['final_val_loss']:.4f}\")\n",
    "\n",
    "# Model architecture\n",
    "print(f\"\\nModel: {checkpoint['model_config']['model_type']}\")\n",
    "print(f\"Depth: {checkpoint['model_config']['depth']}, Width: {checkpoint['model_config']['width']}\")\n",
    "print(f\"Activation: {checkpoint['model_config']['activation']}\")\n",
    "print(f\"Loss function: {checkpoint['training_config']['loss_function']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9ef6ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, checkpoint = load_probe_model(checkpoint_path, device)\n",
    "model.eval()\n",
    "print(f\"Loaded model from: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a6ca9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test loader → compute predictions/targets → plot\n",
    "\n",
    "# 1) Build test loader from datasets dict\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        datasets[\"test\"][\"anchors\"],\n",
    "        datasets[\"test\"][\"distances\"],\n",
    "        datasets[\"test\"][\"labels\"],\n",
    "        datasets[\"test\"][\"sample_weights\"],\n",
    "    ),\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# 2) Inference to get predictions/targets\n",
    "device = next(model.parameters()).device  # assumes 'model' is already defined\n",
    "all_predictions, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for anchors, distances, labels, weights in test_loader:\n",
    "        preds = model(anchors.to(device)).cpu().squeeze()\n",
    "        all_predictions.append(preds)\n",
    "        all_targets.append(distances.squeeze())\n",
    "\n",
    "all_predictions = torch.cat(all_predictions)\n",
    "all_targets = torch.cat(all_targets)\n",
    "\n",
    "# Masks\n",
    "clean_mask = all_targets == 0\n",
    "adv_mask = all_targets > 0\n",
    "\n",
    "# 3) Scatter: Predicted vs True\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(all_targets[clean_mask], all_predictions[clean_mask], alpha=0.3, s=10, label='Clean', color='tab:blue')\n",
    "plt.scatter(all_targets[adv_mask],   all_predictions[adv_mask],   alpha=0.3, s=10, label='Adversarial', color='tab:red')\n",
    "mx = float(all_targets.max())\n",
    "plt.plot([0, mx], [0, mx], 'k--', lw=1, label='Perfect prediction')\n",
    "plt.xlabel('True Distance'); plt.ylabel('Predicted Distance'); plt.title('Predicted vs True Distances')\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4) Histogram: Overlaid True vs Predicted (shared bins)\n",
    "true_all = all_targets.cpu().numpy().astype(float)\n",
    "pred_all = all_predictions.cpu().numpy().astype(float)\n",
    "xmin, xmax = float(np.min(true_all)), float(np.max(true_all)); xmax = xmax if xmax > xmin else xmin + 1e-6\n",
    "edges = np.linspace(xmin, xmax, 51)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.hist(true_all, bins=edges, alpha=0.6, density=True, label='True', color='tab:blue')\n",
    "plt.hist(pred_all, bins=edges, alpha=0.6, density=True, label='Predicted', color='tab:orange')\n",
    "plt.xlabel('Distance'); plt.ylabel('Density'); plt.title('Distribution of True vs Predicted Distances')\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Metrics: correlation, tolerance accuracy (±0.1/±0.2), and binary classification at threshold\n",
    "# Ensure 1D CPU numpy arrays\n",
    "true = all_targets.view(-1).cpu().numpy().astype(float)\n",
    "pred = all_predictions.view(-1).cpu().numpy().astype(float)\n",
    "\n",
    "# 1) Correlations\n",
    "pearson_r, pearson_p = pearsonr(true, pred)\n",
    "spearman_r, spearman_p = spearmanr(true, pred)\n",
    "# Optional R^2\n",
    "ss_res = np.sum((true - pred) ** 2)\n",
    "ss_tot = np.sum((true - true.mean()) ** 2) + 1e-12\n",
    "r2 = 1.0 - ss_res / ss_tot\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CORRELATIONS\")\n",
    "print(f\"Pearson r:  {pearson_r:.4f} (p={pearson_p:.2e})\")\n",
    "print(f\"Spearman r: {spearman_r:.4f} (p={spearman_p:.2e})\")\n",
    "print(f\"R^2:        {r2:.4f}\")\n",
    "\n",
    "# 2) AUROC \n",
    "adv_labels = (true > 0).astype(int)  # 1 for adversarial, 0 for clean\n",
    "roc_auc = roc_auc_score(adv_labels, pred)  # Higher distance = more adversarial\n",
    "print(f\"\\nAUROC (adversarial as positive): {roc_auc:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2) Tolerance accuracy (within ±eps of true)\n",
    "def within_tolerance_acc(true_arr, pred_arr, eps):\n",
    "    return float(np.mean(np.abs(pred_arr - true_arr) <= eps))\n",
    "\n",
    "acc_01 = within_tolerance_acc(true, pred, 0.10)\n",
    "acc_02 = within_tolerance_acc(true, pred, 0.20)\n",
    "\n",
    "print(\"\\nTOLERANCE ACCURACY\")\n",
    "print(f\"Within ±0.10: {acc_01:.4f}\")\n",
    "print(f\"Within ±0.20: {acc_02:.4f}\")\n",
    "\n",
    "# Also show per-group (clean vs adv) tolerance accuracy\n",
    "clean_mask = (all_targets == 0).cpu().numpy()\n",
    "adv_mask   = (all_targets > 0).cpu().numpy()\n",
    "\n",
    "if clean_mask.any():\n",
    "    acc_01_clean = within_tolerance_acc(true[clean_mask], pred[clean_mask], 0.10)\n",
    "    acc_02_clean = within_tolerance_acc(true[clean_mask], pred[clean_mask], 0.20)\n",
    "    print(f\"Within ±0.10 (clean): {acc_01_clean:.4f}\")\n",
    "    print(f\"Within ±0.20 (clean): {acc_02_clean:.4f}\")\n",
    "\n",
    "if adv_mask.any():\n",
    "    acc_01_adv = within_tolerance_acc(true[adv_mask], pred[adv_mask], 0.10)\n",
    "    acc_02_adv = within_tolerance_acc(true[adv_mask], pred[adv_mask], 0.20)\n",
    "    print(f\"Within ±0.10 (adv):   {acc_01_adv:.4f}\")\n",
    "    print(f\"Within ±0.20 (adv):   {acc_02_adv:.4f}\")\n",
    "\n",
    "# 3) Binary classification with threshold on predicted distance\n",
    "#    Predict \"clean\" when pred < threshold; true clean when true == 0\n",
    "threshold = 0.10  # change as needed\n",
    "pred_labels = (pred < threshold).astype(int)\n",
    "true_labels = (true == 0).astype(int)\n",
    "\n",
    "acc = (pred_labels == true_labels).mean()\n",
    "tp = int(((pred_labels == 1) & (true_labels == 1)).sum())\n",
    "fp = int(((pred_labels == 1) & (true_labels == 0)).sum())\n",
    "tn = int(((pred_labels == 0) & (true_labels == 0)).sum())\n",
    "fn = int(((pred_labels == 0) & (true_labels == 1)).sum())\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "print(\"\\nBINARY CLASSIFICATION (predict clean if pred < threshold)\")\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1:        {f1:.4f}\")\n",
    "print(f\"Confusion Matrix: TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8228b",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5afa7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, checkpoint = load_probe_model(checkpoint_path, device)\n",
    "\n",
    "print(checkpoint['model_config'])\n",
    "print(checkpoint['training_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0c7cddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save results to CSV (create/append and back up existing) ===\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Absolute paths as requested\n",
    "results_dir = Path('/rds/general/user/nk1924/home/NADF/data/results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = results_dir / 'metrics.csv'\n",
    "model, checkpoint = load_probe_model(checkpoint_path, device)\n",
    "\n",
    "\n",
    "# Try to gather relevant hyperparameters from globals/model\n",
    "g = globals()\n",
    "def _first(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "# Safely check for model without causing NameError\n",
    "width = checkpoint['model_config']['width']\n",
    "depth = checkpoint['model_config']['depth']\n",
    "upweight = checkpoint['training_config']['upweight_factor']\n",
    "\n",
    "model_class = checkpoint['model_config']['model_type']\n",
    "\n",
    "lr = checkpoint['training_config']['learning_rate']\n",
    "weight_decay = checkpoint['training_config']['weight_decay']\n",
    "batch_size_used = checkpoint['training_config']['batch_size']\n",
    "epochs = checkpoint['training_config']['num_epochs']\n",
    "loss_function = checkpoint['training_config']['loss_function']\n",
    "\n",
    "# Try to get losses from multiple possible sources\n",
    "train_loss = None\n",
    "val_loss = None\n",
    "test_loss = None\n",
    "\n",
    "# Source 1: final_metrics dictionary (from train_regression_model return)\n",
    "if 'final_metrics' in g:\n",
    "    train_loss = g['final_metrics'].get('final_train_loss')\n",
    "    val_loss = g['final_metrics'].get('final_val_loss')\n",
    "    test_loss = g['final_metrics'].get('final_test_loss')\n",
    "\n",
    "# Source 2: checkpoint['training_config']\n",
    "if train_loss is None and 'checkpoint' in g:\n",
    "    tc = g['checkpoint'].get('training_config', {})\n",
    "    train_loss = _first(tc.get('final_train_loss'), tc.get('train_loss'))\n",
    "    val_loss = _first(tc.get('final_val_loss'), tc.get('val_loss'), tc.get('best_val_loss'))\n",
    "    test_loss = _first(tc.get('final_test_loss'), tc.get('test_loss'))\n",
    "\n",
    "# Source 3: Individual global variables\n",
    "if train_loss is None:\n",
    "    train_loss = _first(g.get('train_loss'), g.get('final_train_loss'))\n",
    "if val_loss is None:\n",
    "    val_loss = _first(g.get('val_loss'), g.get('final_val_loss'), g.get('best_val_loss'))\n",
    "if test_loss is None:\n",
    "    test_loss = _first(g.get('test_loss'), g.get('final_test_loss'))\n",
    "\n",
    "# Safely get true and pred arrays from globals\n",
    "if 'true' in g and 'pred' in g:\n",
    "    true_arr = g['true']\n",
    "    pred_arr = g['pred']\n",
    "elif 'all_targets' in g and 'all_predictions' in g:\n",
    "    true_arr = g['all_targets'].view(-1).cpu().numpy().astype(float)\n",
    "    pred_arr = g['all_predictions'].view(-1).cpu().numpy().astype(float)\n",
    "else:\n",
    "    raise NameError(\"Neither 'true'/'pred' nor 'all_targets'/'all_predictions' are defined. Please run the metrics section first.\")\n",
    "\n",
    "# Class distribution (ground-truth): clean (true==0) vs adversarial (true>0)\n",
    "n_clean = int((true_arr == 0).sum())\n",
    "n_adv = int((true_arr > 0).sum())\n",
    "\n",
    "# Get n_samples\n",
    "n_samples = len(true_arr)\n",
    "\n",
    "# Safely get metrics from globals (they should exist if true/pred exist)\n",
    "if 'pearson_r' not in g or 'acc_01' not in g:\n",
    "    raise NameError(\"Metrics variables (pearson_r, acc_01, etc.) not found. Please run the metrics section first.\")\n",
    "\n",
    "row = {\n",
    "    'timestamp': datetime.now().isoformat(timespec='seconds'),\n",
    "    'model_class': model_class,\n",
    "    'width': width,\n",
    "    'depth': depth,\n",
    "    'loss_function': loss_function,\n",
    "    'upweight': upweight,\n",
    "    'lr': lr,\n",
    "    'weight_decay': weight_decay,\n",
    "    'batch_size': batch_size_used,\n",
    "    'epochs': epochs,\n",
    "    'n_samples': int(n_samples),\n",
    "\n",
    "    # target class info (integer)\n",
    "    'target_class': target_class,  # positive class used in metrics (true == 0 → labeled 1)\n",
    "    'n_clean': n_clean,\n",
    "    'n_adv': n_adv,\n",
    "\n",
    "    # losses\n",
    "    'train_loss': float(train_loss) if train_loss is not None else None,\n",
    "    'val_loss': float(val_loss) if val_loss is not None else None,\n",
    "    'test_loss': float(test_loss) if test_loss is not None else None,\n",
    "\n",
    "    # correlations\n",
    "    'pearson_r': float(g['pearson_r']),\n",
    "    'spearman_r': float(g['spearman_r']),\n",
    "    'r2': float(g['r2']),\n",
    "\n",
    "    # tolerance accuracies\n",
    "    'acc_tol_0.10': float(g['acc_01']),\n",
    "    'acc_tol_0.20': float(g['acc_02']),\n",
    "    'acc_tol_0.10_clean': float(g['acc_01_clean']) if ('acc_01_clean' in g and g['acc_01_clean'] is not None) else None,\n",
    "    'acc_tol_0.20_clean': float(g['acc_02_clean']) if ('acc_02_clean' in g and g['acc_02_clean'] is not None) else None,\n",
    "    'acc_tol_0.10_adv': float(g['acc_01_adv']) if ('acc_01_adv' in g and g['acc_01_adv'] is not None) else None,\n",
    "    'acc_tol_0.20_adv': float(g['acc_02_adv']) if ('acc_02_adv' in g and g['acc_02_adv'] is not None) else None,\n",
    "\n",
    "    # AUROC\n",
    "    'auroc': float(g['roc_auc']),\n",
    "\n",
    "    # binary classification\n",
    "    'threshold': float(g['threshold']),\n",
    "    'binary_acc': float(g['acc']),\n",
    "    'precision': float(g['precision']),\n",
    "    'recall': float(g['recall']),\n",
    "    'f1': float(g['f1']),\n",
    "    'tp': int(g['tp']),\n",
    "    'fp': int(g['fp']),\n",
    "    'fn': int(g['fn']),\n",
    "    'tn': int(g['tn']),\n",
    "}\n",
    "\n",
    "# Backup existing CSV as a copy (timestamped) before writing\n",
    "# backup_path = None\n",
    "# if csv_path.exists():\n",
    "#     backup_path = results_dir / f'metrics_copy_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "#     shutil.copy2(csv_path, backup_path)\n",
    "\n",
    "# # Create new or append to existing, preserving all columns across runs\n",
    "# new_row_df = pd.DataFrame([row])\n",
    "# if csv_path.exists():\n",
    "#     existing_df = pd.read_csv(csv_path)\n",
    "#     # Union columns to avoid dropping any\n",
    "#     full_cols = list(dict.fromkeys(list(existing_df.columns) + list(new_row_df.columns)))\n",
    "#     existing_df = existing_df.reindex(columns=full_cols)\n",
    "#     new_row_df = new_row_df.reindex(columns=full_cols)\n",
    "#     out_df = pd.concat([existing_df, new_row_df], ignore_index=True)\n",
    "# else:\n",
    "#     out_df = new_row_df\n",
    "\n",
    "# out_df.to_csv(csv_path, index=False)\n",
    "# print(f\"Saved metrics to {csv_path}\")\n",
    "# if backup_path is not None:\n",
    "#     print(f\"Backup created at {backup_path}\")\n",
    "\n",
    "\n",
    "# Backup existing CSV as a copy (timestamped) before writing\n",
    "backup_path = None\n",
    "if csv_path.exists():\n",
    "    backup_path = results_dir / f'metrics_copy_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    shutil.copy2(csv_path, backup_path)\n",
    "\n",
    "# Create new or replace existing row if match found, preserving all columns across runs\n",
    "new_row_df = pd.DataFrame([row])\n",
    "if csv_path.exists():\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    # Union columns to avoid dropping any\n",
    "    full_cols = list(dict.fromkeys(list(existing_df.columns) + list(new_row_df.columns)))\n",
    "    existing_df = existing_df.reindex(columns=full_cols)\n",
    "    new_row_df = new_row_df.reindex(columns=full_cols)\n",
    "    \n",
    "    # Columns to match on for replacement\n",
    "    match_cols = ['model_class', 'width', 'depth', 'lr', 'weight_decay', \n",
    "                  'batch_size', 'epochs', 'n_samples', 'target_class']\n",
    "    \n",
    "    # Check if a matching row exists\n",
    "    match_mask = None\n",
    "    for col in match_cols:\n",
    "        if col in existing_df.columns and col in new_row_df.columns:\n",
    "            # Get the new value\n",
    "            new_val = new_row_df[col].iloc[0]\n",
    "            \n",
    "            # Compare column values, handling NaN properly\n",
    "            if pd.isna(new_val):\n",
    "                col_match = existing_df[col].isna()\n",
    "            else:\n",
    "                col_match = (existing_df[col] == new_val)\n",
    "            \n",
    "            # Initialize or combine with previous matches\n",
    "            if match_mask is None:\n",
    "                match_mask = col_match\n",
    "            else:\n",
    "                match_mask = match_mask & col_match\n",
    "        else:\n",
    "            # If column missing, no match possible\n",
    "            match_mask = pd.Series([False] * len(existing_df))\n",
    "            break\n",
    "    \n",
    "    if match_mask is not None and match_mask.any():\n",
    "        # Replace the matching row(s) - take first match if multiple\n",
    "        match_idx = match_mask.idxmax()\n",
    "        existing_df.loc[match_idx] = new_row_df.iloc[0]\n",
    "        out_df = existing_df\n",
    "        print(f\"Replaced existing row at index {match_idx}\")\n",
    "    else:\n",
    "        # Append new row if no match found\n",
    "        out_df = pd.concat([existing_df, new_row_df], ignore_index=True)\n",
    "        print(\"Appended new row (no matching row found)\")\n",
    "else:\n",
    "    out_df = new_row_df\n",
    "    print(\"Created new CSV file\")\n",
    "\n",
    "out_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved metrics to {csv_path}\")\n",
    "if backup_path is not None:\n",
    "    print(f\"Backup created at {backup_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42342539",
   "metadata": {},
   "source": [
    "## inspect metrics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "48a51783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the metrics file\n",
    "metrics_path = '/rds/general/user/nk1924/home/NADF/data/results/metrics.csv'\n",
    "metrics_df = pd.read_csv(metrics_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8b0c952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "82232f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df[metrics_df[\"acc_tol_0.20\"] != 1.0]\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "93472105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each target class print row with max acc_tol_0.20\n",
    "metrics_df[metrics_df[\"target_class\"] == -1].sort_values(by=\"acc_tol_0.20\", ascending=False).head(1)[[\"acc_tol_0.20\", \"loss_function\", \"upweight\", \"width\", \"depth\", \"pearson_r\", \"acc_tol_0.10\", \"acc_tol_0.20\", \"acc_tol_0.10_clean\", \"acc_tol_0.20_clean\", \"acc_tol_0.10_adv\", \"acc_tol_0.20_adv\"    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "43531baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[metrics_df[\"target_class\"] == 0].sort_values(by=\"acc_tol_0.20\", ascending=False).head(1)[[\"acc_tol_0.20\", \"loss_function\", \"upweight\", \"width\", \"depth\", \"pearson_r\", \"acc_tol_0.10\", \"acc_tol_0.20\", \"acc_tol_0.10_clean\", \"acc_tol_0.20_clean\", \"acc_tol_0.10_adv\", \"acc_tol_0.20_adv\"    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "947212f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[metrics_df[\"target_class\"] == 1].sort_values(by=\"acc_tol_0.20\", ascending=False).head(1)[[\"acc_tol_0.20\", \"loss_function\", \"upweight\", \"width\", \"depth\", \"pearson_r\", \"acc_tol_0.10\", \"acc_tol_0.20\", \"acc_tol_0.10_clean\", \"acc_tol_0.20_clean\", \"acc_tol_0.10_adv\", \"acc_tol_0.20_adv\"    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3b2c6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[metrics_df[\"target_class\"] == 9].sort_values(by=\"acc_tol_0.20\", ascending=False).head(1)[[\"acc_tol_0.20\", \"loss_function\", \"upweight\", \"width\", \"depth\", \"pearson_r\", \"acc_tol_0.10\", \"acc_tol_0.20\", \"acc_tol_0.10_clean\", \"acc_tol_0.20_clean\", \"acc_tol_0.10_adv\", \"acc_tol_0.20_adv\"    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[[\"loss_function\", \"upweight\", \"width\", \"depth\", \"target_class\", \"pearson_r\", \"acc_tol_0.10\", \"acc_tol_0.20\", \"acc_tol_0.10_clean\", \"acc_tol_0.20_clean\", \"acc_tol_0.10_adv\", \"acc_tol_0.20_adv\", \"roc_auc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "92eb16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows with acc_tol_0.20== 0\n",
    "metrics_df = metrics_df[metrics_df[\"acc_tol_0.20\"] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e48f7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[metrics_df[\"target_class\"] == -1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
