{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac9d94",
   "metadata": {},
   "source": [
    "# NADF Training Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for training Neural Adversarial Distance Functions (NADF).\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. **Setup**: Configure paths and parameters\n",
    "2. **Generate/Load Adversarial Data**: Create or load PGD attacks\n",
    "3. **Train Probe**: Train the NADF probe model with optional augmentation and upweighting\n",
    "4. **Evaluate**: Assess probe performance\n",
    "\n",
    "Run cells in order for the complete workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b18c9",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import libraries and configure training parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2731e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x14786058e610>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/nk1924/home/NADF/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from nadf.data.adversarial import load_or_create_dataset\n",
    "\n",
    "# from nadf.data.datasets import create_training_datasets\n",
    "# from nadf.training.pipeline import apply_augmentation, save_augmented_dataset, train_probe_model\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Environment setup complete!\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"  Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf63fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NADF Probe Training Configuration\n",
      "============================================================\n",
      "Model: mlp, depth=5, width=256\n",
      "Upweighting: 1.0x, Loss: mse\n",
      "Augmentation: none\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Edit these parameters as needed\n",
    "args = SimpleNamespace(\n",
    "    # Data paths\n",
    "    data_folder=os.getenv(\"RESNET_MODEL_FOLDER\"),\n",
    "    cache_folder=\"/rds/general/user/nk1924/home/nadf/data\",\n",
    "    target_class=-1,\n",
    "    recreate_data=True,\n",
    "    # Model architecture\n",
    "    model_type=\"mlp\",\n",
    "    depth=5,\n",
    "    width=256,\n",
    "    activation=\"relu\",\n",
    "    # Training hyperparameters\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=512,\n",
    "    epochs=100,\n",
    "    # Loss function\n",
    "    loss=\"mse\",\n",
    "    huber_delta=0.05,\n",
    "    # Upweighting and augmentation\n",
    "    upweight=1.0,\n",
    "    augmentation=\"none\",\n",
    "    num_augmentations=1,\n",
    "    # Output\n",
    "    save_dir=\"trained_models\",\n",
    "    checkpoint_name=None,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NADF Probe Training Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {args.model_type}, depth={args.depth}, width={args.width}\")\n",
    "print(f\"Upweighting: {args.upweight}x, Loss: {args.loss}\")\n",
    "print(f\"Augmentation: {args.augmentation}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861a797",
   "metadata": {},
   "source": [
    "## Debug: Check Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESNET_MODEL_FOLDER from .env: /rds/general/user/nk1924/home/shared_models/cifar10/adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0\n",
      "Data folder being used: /rds/general/user/nk1924/home/shared_models/cifar10/adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0\n",
      "\n",
      "Model's saved configuration:\n",
      "  Dataset: cifar10\n",
      "  Data path: /rds/general/user/nk1924/home/shared_models\n",
      "  Path exists: True\n",
      "\n",
      "CIFAR10 data path: /rds/general/user/nk1924/home/shared_models/cifar-10-batches-py\n",
      "CIFAR10 data exists: True\n"
     ]
    }
   ],
   "source": [
    "# Check what path the model is configured to use\n",
    "print(f\"RESNET_MODEL_FOLDER from .env: {os.getenv('RESNET_MODEL_FOLDER')}\")\n",
    "print(f\"Data folder being used: {args.data_folder}\")\n",
    "\n",
    "# Load the model's saved args to see what data path it expects\n",
    "model_args = torch.load(os.path.join(args.data_folder, \"args.info\"), map_location=\"cpu\", weights_only=False)\n",
    "print(\"\\nModel's saved configuration:\")\n",
    "print(f\"  Dataset: {model_args.dataset}\")\n",
    "print(f\"  Data path: {model_args.path}\")\n",
    "print(f\"  Path exists: {os.path.exists(model_args.path)}\")\n",
    "\n",
    "# Check if CIFAR10 data exists at that path\n",
    "cifar10_path = os.path.join(model_args.path, \"cifar-10-batches-py\")\n",
    "print(f\"\\nCIFAR10 data path: {cifar10_path}\")\n",
    "print(f\"CIFAR10 data exists: {os.path.exists(cifar10_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665b6ca",
   "metadata": {},
   "source": [
    "## Fix: Update Model's Data Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Updated model's data path to: /rds/general/user/nk1924/home/shared_models\n",
      "✓ Verifying CIFAR10 data exists...\n",
      "  Checking: /rds/general/user/nk1924/home/shared_models/cifar10/cifar-10-batches-py\n",
      "  Exists: False\n"
     ]
    }
   ],
   "source": [
    "# Update the model's data path to point to where CIFAR10 actually exists\n",
    "model_args.path = \"/rds/general/user/nk1924/home/shared_models\"\n",
    "\n",
    "# Save the updated args back to the model folder\n",
    "torch.save(model_args, os.path.join(args.data_folder, \"args.info\"))\n",
    "\n",
    "print(\"✓ Updated model's data path to:\", model_args.path)\n",
    "print(\"✓ Verifying CIFAR10 data exists...\")\n",
    "\n",
    "cifar10_check = os.path.join(model_args.path, \"cifar10\", \"cifar-10-batches-py\")\n",
    "print(f\"  Checking: {cifar10_check}\")\n",
    "print(f\"  Exists: {os.path.exists(cifar10_check)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6da43",
   "metadata": {},
   "source": [
    "## Debug: Check what's in the cifar10 directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79969d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /rds/general/user/nk1924/home/shared_models/:\n",
      "total 19\n",
      "drwxr-sr-x.  4 nk1924 hpc-tbirdal 4096 Oct 29 11:51 .\n",
      "drwx--s---. 24 nk1924 hpc-tbirdal 8192 Nov 11 15:15 ..\n",
      "drwxr-xr-x. 18 nk1924 hpc-tbirdal 4096 Jul 31 16:48 cifar10\n",
      "drwxr-sr-x.  2 nk1924 hpc-tbirdal 4096 Oct 29 11:51 cifar-10-batches-py\n",
      "\n",
      "\n",
      "Contents of /rds/general/user/nk1924/home/shared_models/cifar10/:\n",
      "total 946\n",
      "drwxr-xr-x. 18 nk1924 hpc-tbirdal  4096 Jul 31 16:48 .\n",
      "drwxr-sr-x.  4 nk1924 hpc-tbirdal  4096 Oct 29 11:51 ..\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 31823 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 43249 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 32470 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38431 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 35379 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 41398 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s1\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 40586 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s1_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s2\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38144 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s2_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s3\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 36802 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s3_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s4\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38036 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s4_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 32352 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  5 nk1924 hpc-tbirdal  4096 Oct  4 20:39 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38422 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s1\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 39518 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s1_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s2\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 44341 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s2_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s3\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 36595 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s3_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s4\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38723 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s4_traj.log\n",
      "\n",
      "\n",
      "Does /rds/general/user/nk1924/home/shared_models/cifar-10-batches-py exist? True\n",
      "Does /rds/general/user/nk1924/home/shared_models/CIFAR10 exist? False\n"
     ]
    }
   ],
   "source": [
    "# Check what's actually in the cifar10 directory\n",
    "import subprocess\n",
    "\n",
    "print(\"Contents of /rds/general/user/nk1924/home/shared_models/:\")\n",
    "result = subprocess.run([\"ls\", \"-la\", \"/rds/general/user/nk1924/home/shared_models/\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"\\nContents of /rds/general/user/nk1924/home/shared_models/cifar10/:\")\n",
    "result = subprocess.run(\n",
    "    [\"ls\", \"-la\", \"/rds/general/user/nk1924/home/shared_models/cifar10/\"], capture_output=True, text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "\n",
    "# Check if maybe it's directly in shared_models without the extra cifar10 subdirectory\n",
    "cifar_batch_path = \"/rds/general/user/nk1924/home/shared_models/cifar-10-batches-py\"\n",
    "print(f\"\\nDoes {cifar_batch_path} exist? {os.path.exists(cifar_batch_path)}\")\n",
    "\n",
    "# Or maybe the pytorch CIFAR10 dataset structure\n",
    "pytorch_cifar_path = \"/rds/general/user/nk1924/home/shared_models/CIFAR10\"\n",
    "print(f\"Does {pytorch_cifar_path} exist? {os.path.exists(pytorch_cifar_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a8a15",
   "metadata": {},
   "source": [
    "## Download CIFAR10 Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb93f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CIFAR10 dataset (this may take a few minutes)...\n",
      "✓ CIFAR10 downloaded successfully to /rds/general/user/nk1924/home/shared_models\n",
      "✓ Train set: 50000 samples\n",
      "✓ Test set: 10000 samples\n",
      "\n",
      "✓ Dataset files exist at: /rds/general/user/nk1924/home/shared_models/cifar-10-batches-py\n",
      "  Verified: True\n"
     ]
    }
   ],
   "source": [
    "# Download CIFAR10 dataset to the shared_models directory\n",
    "# This will bypass SSL verification since we're on a cluster\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "# Download CIFAR10 to the shared_models directory\n",
    "print(\"Downloading CIFAR10 dataset (this may take a few minutes)...\")\n",
    "cifar_download_path = \"/rds/general/user/nk1924/home/shared_models\"\n",
    "\n",
    "# Download train and test sets\n",
    "train_dataset = datasets.CIFAR10(root=cifar_download_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=cifar_download_path, train=False, download=True)\n",
    "\n",
    "print(f\"✓ CIFAR10 downloaded successfully to {cifar_download_path}\")\n",
    "print(f\"✓ Train set: {len(train_dataset)} samples\")\n",
    "print(f\"✓ Test set: {len(test_dataset)} samples\")\n",
    "\n",
    "# Verify it exists\n",
    "cifar_batch_path = os.path.join(cifar_download_path, \"cifar-10-batches-py\")\n",
    "print(f\"\\n✓ Dataset files exist at: {cifar_batch_path}\")\n",
    "print(f\"  Verified: {os.path.exists(cifar_batch_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c175e8",
   "metadata": {},
   "source": [
    "## 2. Generate or Load Adversarial Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c770fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading train data...\n",
      "    train: 45000 examples (filtered), 45000 in pool\n",
      "  Loading val data...\n",
      "    val: 5000 examples (filtered), 5000 in pool\n",
      "  Loading test data...\n",
      "    test: 10000 examples (filtered), 10000 in pool\n",
      "clean_data dict_keys(['train', 'val', 'test'])\n",
      "  Processing train...\n",
      "    Attacks: num=4, eps_coef=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      Attack 1/4: 100%|██████████████████████████████████████████████████████████████████████████████| 88/88 [01:43<00:00,  1.18s/it]\n",
      "      Attack 2/4:  55%|██████████████████████████████████████████▌                                   | 48/88 [00:56<00:46,  1.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_attacks_eps_coef \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0.25\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_create_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_attacks_eps_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_attacks_eps_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecreate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecreate_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_clean\u001b[39m\u001b[38;5;124m'\u001b[39m][split])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clean, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_adv\u001b[39m\u001b[38;5;124m'\u001b[39m][split])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m adversarial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NADF/src/nadf/data/adversarial.py:61\u001b[0m, in \u001b[0;36mload_or_create_dataset\u001b[0;34m(folder, target_class, num_attacks_eps_coef, splits, recreate, verbose)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, clean_data\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Step 2: Generate adversarial examples\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m adv_data \u001b[38;5;241m=\u001b[39m \u001b[43m_generate_adversarial_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_attacks_eps_coef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Step 3: Extract representations\u001b[39;00m\n\u001b[1;32m     66\u001b[0m representations \u001b[38;5;241m=\u001b[39m _extract_representations(model, clean_data, adv_data, splits, device, target_class)\n",
      "File \u001b[0;32m~/NADF/src/nadf/data/adversarial.py:292\u001b[0m, in \u001b[0;36m_generate_adversarial_examples\u001b[0;34m(model, clean_data, num_attacks_eps_coef, splits, device, target_class, verbose)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Generate adversarial examples\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m new_x_adv_batch, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_x_adv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnpize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_clean_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattack_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# CIFAR-10 image shape\u001b[39;49;00m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# num_classes for CIFAR-10\u001b[39;49;00m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSTATS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattack_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meps_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattack_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_attack_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meps_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattacks_max_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_attack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattacker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Filter successful attacks\u001b[39;00m\n\u001b[1;32m    310\u001b[0m idx, stats \u001b[38;5;241m=\u001b[39m filter_successful_attacks(\n\u001b[1;32m    311\u001b[0m     model, x_clean_batch, y_clean_batch, new_x_adv_batch, device, target_class, verbose\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m~/NADF/src/nadf/utils.py:385\u001b[0m, in \u001b[0;36mgenerate_x_adv\u001b[0;34m(net, x_orig, p, input_shape, num_classes, stats, crit, device, attack_eps, attack_lr, attacks_max_iter, seed, attacker, evaluation)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     x_adv \u001b[38;5;241m=\u001b[39m \u001b[43mattacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass NCHW\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     x_adv \u001b[38;5;241m=\u001b[39m (x_adv \u001b[38;5;241m-\u001b[39m x_mean) \u001b[38;5;241m/\u001b[39m x_std\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(x_adv)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mtensor(x_orig)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent.py:202\u001b[0m, in \u001b[0;36mProjectedGradientDescent.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03mGenerate adversarial samples and return them in an array.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m:return: An array holding the adversarial examples.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating adversarial samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:223\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rand_init_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_random_init)):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rand_init_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;66;03m# first iteration: use the adversarial examples as they are the only ones we have now\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m         adv_x[batch_index_1:batch_index_2] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_eps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_eps_step\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m         adversarial_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_batch(\n\u001b[1;32m    228\u001b[0m             x\u001b[38;5;241m=\u001b[39mbatch, targets\u001b[38;5;241m=\u001b[39mbatch_labels, mask\u001b[38;5;241m=\u001b[39mmask_batch, eps\u001b[38;5;241m=\u001b[39mbatch_eps, eps_step\u001b[38;5;241m=\u001b[39mbatch_eps_step\n\u001b[1;32m    229\u001b[0m         )\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:284\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._generate_batch\u001b[0;34m(self, x, targets, mask, eps, eps_step)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_max_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_max_iter \u001b[38;5;241m=\u001b[39m i_max_iter\n\u001b[0;32m--> 284\u001b[0m     adv_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_pytorch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43madv_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_random_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi_max_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_x\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:445\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._compute_pytorch\u001b[0;34m(self, x, x_init, y, mask, eps, eps_step, random_init, momentum)\u001b[0m\n\u001b[1;32m    442\u001b[0m     x_adv \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Get perturbation\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m perturbation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_perturbation_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Apply perturbation and clip\u001b[39;00m\n\u001b[1;32m    448\u001b[0m x_adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_perturbation_pytorch(x_adv, perturbation, eps_step)\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:309\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch._compute_perturbation_pytorch\u001b[0;34m(self, x, y, mask, momentum)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Get gradient wrt loss; invert it if attack is targeted\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargeted \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Write summary\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/art/estimators/classification/pytorch.py:856\u001b[0m, in \u001b[0;36mPyTorchClassifier.loss_gradient\u001b[0;34m(self, x, y, training_mode, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m grads: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_grad\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_attacks_eps_coef = [(4, 0.25), (2, 0.5), (3, 1), (1, 2)]\n",
    "\n",
    "dataset = load_or_create_dataset(\n",
    "    folder=args.data_folder,\n",
    "    target_class=args.target_class,\n",
    "    num_attacks_eps_coef=num_attacks_eps_coef,\n",
    "    splits=[\"train\", \"val\", \"test\"],\n",
    "    recreate=args.recreate_data,\n",
    ")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"{split}: {len(dataset['z_clean'][split])} clean, {len(dataset['z_adv'][split])} adversarial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4befe7",
   "metadata": {},
   "source": [
    "## 3. Apply Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_stats = apply_augmentation(dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754619e5",
   "metadata": {},
   "source": [
    "## 4. Create Training Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674709b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = create_training_datasets(dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac54066",
   "metadata": {},
   "source": [
    "## 5. Save Augmented Dataset (if applicable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade12924",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.augmentation != \"none\" and augmentation_stats:\n",
    "    save_augmented_dataset(dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debd48d",
   "metadata": {},
   "source": [
    "## 6. Train Probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probe_model(datasets, args)\n",
    "print(\"✓ Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
