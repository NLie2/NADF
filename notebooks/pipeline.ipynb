{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac9d94",
   "metadata": {},
   "source": [
    "# NADF Training Pipeline\n",
    "\n",
    "This notebook provides a complete pipeline for training Neural Adversarial Distance Functions (NADF).\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. **Setup**: Configure paths and parameters\n",
    "2. **Generate/Load Adversarial Data**: Create or load PGD attacks\n",
    "3. **Train Probe**: Train the NADF probe model with optional augmentation and upweighting\n",
    "4. **Evaluate**: Assess probe performance\n",
    "\n",
    "Run cells in order for the complete workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b18c9",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import libraries and configure training parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2731e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment setup complete!\n",
      "  CUDA available: False\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from nadf.data.adversarial import load_or_create_dataset\n",
    "# from nadf.data.datasets import create_training_datasets\n",
    "# from nadf.training.pipeline import apply_augmentation, save_augmented_dataset, train_probe_model\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Environment setup complete!\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"  Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf63fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NADF Probe Training Configuration\n",
      "============================================================\n",
      "Model: mlp, depth=5, width=256\n",
      "Upweighting: 1.0x, Loss: mse\n",
      "Augmentation: none\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Edit these parameters as needed\n",
    "args = SimpleNamespace(\n",
    "    # Data paths\n",
    "    data_folder=os.getenv(\"RESNET_MODEL_FOLDER\"),\n",
    "    cache_folder=\"/rds/general/user/nk1924/home/nadf/data\",\n",
    "    target_class=-1,\n",
    "    recreate_data=True,\n",
    "    \n",
    "    # Model architecture\n",
    "    model_type=\"mlp\",\n",
    "    depth=5,\n",
    "    width=256,\n",
    "    activation=\"relu\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=512,\n",
    "    epochs=100,\n",
    "    \n",
    "    # Loss function\n",
    "    loss=\"mse\",\n",
    "    huber_delta=0.05,\n",
    "    \n",
    "    # Upweighting and augmentation\n",
    "    upweight=1.0,\n",
    "    augmentation=\"none\",\n",
    "    num_augmentations=1,\n",
    "    \n",
    "    # Output\n",
    "    save_dir=\"trained_models\",\n",
    "    checkpoint_name=None,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NADF Probe Training Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {args.model_type}, depth={args.depth}, width={args.width}\")\n",
    "print(f\"Upweighting: {args.upweight}x, Loss: {args.loss}\")\n",
    "print(f\"Augmentation: {args.augmentation}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861a797",
   "metadata": {},
   "source": [
    "## Debug: Check Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee76de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESNET_MODEL_FOLDER from .env: /rds/general/user/nk1924/home/shared_models/cifar10/adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0\n",
      "Data folder being used: /rds/general/user/nk1924/home/shared_models/cifar10/adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0\n",
      "\n",
      "Model's saved configuration:\n",
      "  Dataset: cifar10\n",
      "  Data path: ./data\n",
      "  Path exists: True\n",
      "\n",
      "CIFAR10 data path: ./data/cifar-10-batches-py\n",
      "CIFAR10 data exists: False\n"
     ]
    }
   ],
   "source": [
    "# Check what path the model is configured to use\n",
    "print(f\"RESNET_MODEL_FOLDER from .env: {os.getenv('RESNET_MODEL_FOLDER')}\")\n",
    "print(f\"Data folder being used: {args.data_folder}\")\n",
    "\n",
    "# Load the model's saved args to see what data path it expects\n",
    "model_args = torch.load(os.path.join(args.data_folder, \"args.info\"), map_location=\"cpu\", weights_only=False)\n",
    "print(f\"\\nModel's saved configuration:\")\n",
    "print(f\"  Dataset: {model_args.dataset}\")\n",
    "print(f\"  Data path: {model_args.path}\")\n",
    "print(f\"  Path exists: {os.path.exists(model_args.path)}\")\n",
    "\n",
    "# Check if CIFAR10 data exists at that path\n",
    "cifar10_path = os.path.join(model_args.path, \"cifar-10-batches-py\")\n",
    "print(f\"\\nCIFAR10 data path: {cifar10_path}\")\n",
    "print(f\"CIFAR10 data exists: {os.path.exists(cifar10_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665b6ca",
   "metadata": {},
   "source": [
    "## Fix: Update Model's Data Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dce58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Updated model's data path to: /rds/general/user/nk1924/home/shared_models\n",
      "✓ Verifying CIFAR10 data exists...\n",
      "  Checking: /rds/general/user/nk1924/home/shared_models/cifar10/cifar-10-batches-py\n",
      "  Exists: False\n"
     ]
    }
   ],
   "source": [
    "# Update the model's data path to point to where CIFAR10 actually exists\n",
    "model_args.path = \"/rds/general/user/nk1924/home/shared_models\"\n",
    "\n",
    "# Save the updated args back to the model folder\n",
    "torch.save(model_args, os.path.join(args.data_folder, \"args.info\"))\n",
    "\n",
    "print(\"✓ Updated model's data path to:\", model_args.path)\n",
    "print(\"✓ Verifying CIFAR10 data exists...\")\n",
    "\n",
    "cifar10_check = os.path.join(model_args.path, \"cifar10\", \"cifar-10-batches-py\")\n",
    "print(f\"  Checking: {cifar10_check}\")\n",
    "print(f\"  Exists: {os.path.exists(cifar10_check)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6da43",
   "metadata": {},
   "source": [
    "## Debug: Check what's in the cifar10 directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79969d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /rds/general/user/nk1924/home/shared_models/:\n",
      "total 3\n",
      "drwxr-sr-x.  3 nk1924 hpc-tbirdal 4096 Aug  1 13:22 .\n",
      "drwx--s---. 26 nk1924 hpc-tbirdal 4096 Oct 28 13:09 ..\n",
      "drwxr-xr-x. 18 nk1924 hpc-tbirdal 4096 Jul 31 16:48 cifar10\n",
      "\n",
      "\n",
      "Contents of /rds/general/user/nk1924/home/shared_models/cifar10/:\n",
      "total 946\n",
      "drwxr-xr-x. 18 nk1924 hpc-tbirdal  4096 Jul 31 16:48 .\n",
      "drwxr-sr-x.  3 nk1924 hpc-tbirdal  4096 Aug  1 13:22 ..\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 31823 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 43249 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 32470 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38431 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.001_resnet18repr-128_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 35379 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 41398 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s1\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 40586 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s1_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s2\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38144 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s2_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s3\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 36802 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s3_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s4\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38036 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18_cifar10_lr0.001_s4_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.0001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 32352 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.0001_s0_traj.log\n",
      "drwxr-xr-x.  5 nk1924 hpc-tbirdal  4096 Oct  4 20:39 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38422 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s0_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s1\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 39518 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s1_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s2\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 44341 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s2_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s3\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 36595 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s3_traj.log\n",
      "drwxr-xr-x.  4 nk1924 hpc-tbirdal  4096 Jul 31 16:46 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s4\n",
      "-rw-r--r--.  1 nk1924 hpc-tbirdal 38723 Jul 31 16:48 adam_augbasic_cosine-60-0.0_wd0.01_resnet18repr-128_cifar10_lr0.001_s4_traj.log\n",
      "\n",
      "\n",
      "Does /rds/general/user/nk1924/home/shared_models/cifar-10-batches-py exist? False\n",
      "Does /rds/general/user/nk1924/home/shared_models/CIFAR10 exist? False\n"
     ]
    }
   ],
   "source": [
    "# Check what's actually in the cifar10 directory\n",
    "import subprocess\n",
    "\n",
    "print(\"Contents of /rds/general/user/nk1924/home/shared_models/:\")\n",
    "result = subprocess.run(['ls', '-la', '/rds/general/user/nk1924/home/shared_models/'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"\\nContents of /rds/general/user/nk1924/home/shared_models/cifar10/:\")\n",
    "result = subprocess.run(['ls', '-la', '/rds/general/user/nk1924/home/shared_models/cifar10/'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Check if maybe it's directly in shared_models without the extra cifar10 subdirectory\n",
    "cifar_batch_path = \"/rds/general/user/nk1924/home/shared_models/cifar-10-batches-py\"\n",
    "print(f\"\\nDoes {cifar_batch_path} exist? {os.path.exists(cifar_batch_path)}\")\n",
    "\n",
    "# Or maybe the pytorch CIFAR10 dataset structure\n",
    "pytorch_cifar_path = \"/rds/general/user/nk1924/home/shared_models/CIFAR10\"\n",
    "print(f\"Does {pytorch_cifar_path} exist? {os.path.exists(pytorch_cifar_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a8a15",
   "metadata": {},
   "source": [
    "## Download CIFAR10 Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb93f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR10 dataset to the shared_models directory\n",
    "# This will bypass SSL verification since we're on a cluster\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "# Download CIFAR10 to the shared_models directory\n",
    "print(\"Downloading CIFAR10 dataset (this may take a few minutes)...\")\n",
    "cifar_download_path = \"/rds/general/user/nk1924/home/shared_models\"\n",
    "\n",
    "# Download train and test sets\n",
    "train_dataset = datasets.CIFAR10(root=cifar_download_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=cifar_download_path, train=False, download=True)\n",
    "\n",
    "print(f\"✓ CIFAR10 downloaded successfully to {cifar_download_path}\")\n",
    "print(f\"✓ Train set: {len(train_dataset)} samples\")\n",
    "print(f\"✓ Test set: {len(test_dataset)} samples\")\n",
    "\n",
    "# Verify it exists\n",
    "cifar_batch_path = os.path.join(cifar_download_path, \"cifar-10-batches-py\")\n",
    "print(f\"\\n✓ Dataset files exist at: {cifar_batch_path}\")\n",
    "print(f\"  Verified: {os.path.exists(cifar_batch_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c175e8",
   "metadata": {},
   "source": [
    "## 2. Generate or Load Adversarial Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c770fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading train data...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_attacks_eps_coef \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0.25\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_create_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_attacks_eps_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_attacks_eps_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecreate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecreate_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_clean\u001b[39m\u001b[38;5;124m'\u001b[39m][split])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clean, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz_adv\u001b[39m\u001b[38;5;124m'\u001b[39m][split])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m adversarial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NADF/src/nadf/data/adversarial.py:52\u001b[0m, in \u001b[0;36mload_or_create_dataset\u001b[0;34m(folder, target_class, num_attacks_eps_coef, splits, recreate)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_cached_dataset(cache_path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Step 1: Load model and clean data\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m model, clean_data, device \u001b[38;5;241m=\u001b[39m \u001b[43m_load_model_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Step 2: Generate adversarial examples\u001b[39;00m\n\u001b[1;32m     55\u001b[0m adv_data \u001b[38;5;241m=\u001b[39m _generate_adversarial_examples(model, clean_data, num_attacks_eps_coef, splits, device)\n",
      "File \u001b[0;32m~/NADF/src/nadf/data/adversarial.py:84\u001b[0m, in \u001b[0;36m_load_model_and_data\u001b[0;34m(folder, splits, target_class)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Get data for this split\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_details\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_info\u001b[49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_samples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_hist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Filter by target class\u001b[39;00m\n\u001b[1;32m     93\u001b[0m idx \u001b[38;5;241m=\u001b[39m (r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m target_class)\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mif\u001b[39;00m target_class \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/NADF/src/nadf/utils.py:287\u001b[0m, in \u001b[0;36mget_model_details\u001b[0;34m(folder, get_model, model_iter, bias, get_hist, get_data_loaders, get_samples, split, num_samples, layer_idx, seed, device, repr_eval_mode)\u001b[0m\n\u001b[1;32m    284\u001b[0m     net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_data_loaders:\n\u001b[0;32m--> 287\u001b[0m     r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loader\u001b[39m\u001b[38;5;124m\"\u001b[39m], r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loaders\u001b[39m\u001b[38;5;124m\"\u001b[39m], r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m], r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_biased\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_samples:\n\u001b[1;32m    292\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loaders\u001b[39m\u001b[38;5;124m\"\u001b[39m][split]\n",
      "File \u001b[0;32m~/NADF/src/nadf/utils.py:122\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(args, return_biased, abtrain, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m split \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvhn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Set download=False to avoid SSL issues on cluster where data already exists\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m te_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m tr_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    127\u001b[0m val_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NADF/.venv/lib/python3.9/site-packages/torchvision/datasets/cifar.py:69\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[1;32m     72\u001b[0m     downloaded_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_list\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "num_attacks_eps_coef = [(4, 0.25), (2, 0.5), (3, 1), (1, 2)]\n",
    "\n",
    "dataset = load_or_create_dataset(\n",
    "    folder=args.data_folder,\n",
    "    target_class=args.target_class,\n",
    "    num_attacks_eps_coef=num_attacks_eps_coef,\n",
    "    splits=[\"train\", \"val\", \"test\"],\n",
    "    recreate=args.recreate_data,\n",
    ")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    print(f\"{split}: {len(dataset['z_clean'][split])} clean, {len(dataset['z_adv'][split])} adversarial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4befe7",
   "metadata": {},
   "source": [
    "## 3. Apply Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_stats = apply_augmentation(dataset, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754619e5",
   "metadata": {},
   "source": [
    "## 4. Create Training Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674709b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = create_training_datasets(dataset, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac54066",
   "metadata": {},
   "source": [
    "## 5. Save Augmented Dataset (if applicable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade12924",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.augmentation != \"none\" and augmentation_stats:\n",
    "    save_augmented_dataset(dataset, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debd48d",
   "metadata": {},
   "source": [
    "## 6. Train Probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probe_model(datasets, args)\n",
    "print(\"✓ Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
